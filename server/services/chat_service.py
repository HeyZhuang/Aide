# services/chat_service.py

# Import necessary modules
import asyncio
import json
from typing import Dict, Any, List, Optional

# Import service modules
from models.tool_model import ToolInfoJson
from services.db_service import db_service
from services.langgraph_service import langgraph_multi_agent
from services.websocket_service import send_to_websocket
from services.stream_service import add_stream_task, remove_stream_task
from models.config_model import ModelInfo


async def handle_chat(data: Dict[str, Any]) -> None:
    """
    Handle an incoming chat request.

    Workflow:
    - Parse incoming chat data.
    - Optionally inject system prompt.
    - Save chat session and messages to the database.
    - Launch langgraph_agent task to process chat.
    - Manage stream task lifecycle (add, remove).
    - Notify frontend via WebSocket when stream is done.

    Args:
        data (dict): Chat request data containing:
            - messages: list of message dicts
            - session_id: unique session identifier
            - canvas_id: canvas identifier (contextual use)
            - text_model: text model configuration
            - tool_list: list of tool model configurations (images/videos)
    """
    print('='*80)
    print('ğŸ“¥ [å‰ç«¯è¯·æ±‚] æ”¶åˆ° /api/chat è¯·æ±‚')
    print('='*80)

    # Extract fields from incoming data
    messages: List[Dict[str, Any]] = data.get('messages', [])
    session_id: str = data.get('session_id', '')
    canvas_id: str = data.get('canvas_id', '')
    text_model: ModelInfo = data.get('text_model', {})
    tool_list: List[ToolInfoJson] = data.get('tool_list', [])

    print(f'ğŸ“‹ session_id: {session_id}')
    print(f'ğŸ“‹ canvas_id: {canvas_id}')
    print(f'ğŸ“‹ messages æ•°é‡: {len(messages)}')
    if messages:
        last_msg = messages[-1]
        print(f'ğŸ“‹ æœ€åä¸€æ¡æ¶ˆæ¯: role={last_msg.get("role")}, content={str(last_msg.get("content"))[:100]}...')
    print(f'ğŸ“‹ åŸå§‹ text_model: {text_model}')
    print(f'ğŸ“‹ tool_list: {tool_list}')

    # TODO: [Geminié›†æˆ] è‡ªåŠ¨ä½¿ç”¨ Gemini ä½œä¸ºé»˜è®¤ planner æ¨¡å‹
    # åŸå› : é¿å…ä¾èµ– OpenAI/jaaz API Key é…ç½®
    # å½“ text_model æœªæŒ‡å®šæˆ–ä½¿ç”¨ openai/jaaz æ—¶ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ° Gemini
    # ä¿®å¤æ­¥éª¤: å¦‚æœéœ€è¦æ¢å¤ä½¿ç”¨ OpenAIï¼Œç¡®ä¿ config.toml ä¸­ [openai] æˆ– [jaaz] çš„ api_key æœ‰æ•ˆ
    # å½±å“èŒƒå›´: LangGraph planner ä½¿ç”¨ Gemini è¿›è¡Œå·¥å…·é€‰æ‹©å’Œæ¨ç†
    if not text_model or not text_model.get('model') or text_model.get('provider') in ['openai', 'jaaz']:
        from services.config_service import config_service
        gemini_config = config_service.app_config.get('gemini', {})
        if gemini_config.get('api_key'):
            # TODO: [Geminiæ¨¡å‹é€‰æ‹©] ä½¿ç”¨ gemini-2.5-flash æ›¿ä»£å®éªŒæ€§çš„ 2.0-flash-exp
            # åŸå› : gemini-2.0-flash-exp å¯èƒ½ä¸ç¨³å®šï¼Œå¯¼è‡´å·¥å…·è°ƒç”¨é—®é¢˜
            # å¦‚éœ€åˆ‡æ¢å› 2.0-flash-expï¼Œä¿®æ”¹ä¸‹é¢çš„ model å€¼
            text_model = {
                'model': 'gemini-2.5-flash',  # ä½¿ç”¨ç¨³å®šç‰ˆæœ¬
                'provider': 'gemini',
                'url': gemini_config.get('url', 'https://generativelanguage.googleapis.com/v1beta'),
                'max_tokens': 8192
            }
            print(f'ğŸ”„ [è‡ªåŠ¨åˆ‡æ¢] ä½¿ç”¨ Gemini ä½œä¸º planner æ¨¡å‹: {text_model["model"]}')

    print(f'âœ… æœ€ç»ˆ text_model: {text_model}')
    print('='*80)

    # TODO: save and fetch system prompt from db or settings config
    system_prompt: Optional[str] = data.get('system_prompt')

    # If there is only one message, create a new chat session
    if len(messages) == 1:
        # create new session
        prompt = messages[0].get('content', '')
        # TODO: Better way to determin when to create new chat session.
        await db_service.create_chat_session(session_id, text_model.get('model'), text_model.get('provider'), canvas_id, (prompt[:200] if isinstance(prompt, str) else ''))

    await db_service.create_message(session_id, messages[-1].get('role', 'user'), json.dumps(messages[-1])) if len(messages) > 0 else None

    # Create and start langgraph_agent task for chat processing
    print(f'ğŸš€ [å¯åŠ¨ä»»åŠ¡] å¼€å§‹ LangGraph å¤šæ™ºèƒ½ä½“å¤„ç†')
    task = asyncio.create_task(langgraph_multi_agent(
        messages, canvas_id, session_id, text_model, tool_list, system_prompt))

    # Register the task in stream_tasks (for possible cancellation)
    add_stream_task(session_id, task)
    try:
        # Await completion of the langgraph_agent task
        print(f'â³ [ç­‰å¾…å“åº”] ç­‰å¾… LangGraph è¿”å›ç»“æœ...')
        await task
        print(f'âœ… [ä»»åŠ¡å®Œæˆ] LangGraph å¤„ç†å®Œæˆ')
    except asyncio.exceptions.CancelledError:
        print(f"ğŸ›‘ [ä»»åŠ¡å–æ¶ˆ] Session {session_id} cancelled during stream")
    except Exception as e:
        print(f"âŒ [ä»»åŠ¡é”™è¯¯] {type(e).__name__}: {str(e)}")
        import traceback
        traceback.print_exc()
    finally:
        # Always remove the task from stream_tasks after completion/cancellation
        remove_stream_task(session_id)
        # Notify frontend WebSocket that chat processing is done
        print(f'ğŸ“¤ [å‘é€å®Œæˆ] é€šçŸ¥å‰ç«¯ä»»åŠ¡å®Œæˆ')
        await send_to_websocket(session_id, {
            'type': 'done'
        })
        print('='*80)
